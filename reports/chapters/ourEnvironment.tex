Our flatland environment is based on RailEnv where we wrapped the main functionalities, methods and we added parameters in order to personalizehave parameterized it through file yaml \textit{"env$\_$parameters.yml"}. 
We have wrapped and added these main functionalities:
\begin{itemize}
\item \textbf{reset:}
\item \textbf{step:}
\item \textbf{render env:}
\item \textbf{encode info:}
\end{itemize}	
\section{Parameters}
\section{Metrics}
\section{Evaluation and platforms}
\section{Action}
The Flatland environment provides for each agent five different actions that they are described in \hyperref[sec:envActions]{2.3}. \\ 
The DO NOTHING is not necessary to reach a solution, because the agent can continue moving forward deciding each time the action MOVE FORWARD or stop using STOP MOVING. As we think that further from being useless it may also damage the overall performance we decided to consider its removal. It has already mentioned the ambiguity of the actions MOVE LEFT and MOVE RIGHT where they are forbidden, it is natural to conclude that the agents may learn bad policies that maps these actions to the same effect of the MOVE FORWARD action. We observed that this phenomenon is very common due to the presence of long straight paths where the agent is allowed only to stop or move forward and concluded that even stopping in the middle of the rail does not have much sense because agents still stop when other agents block their way due to deadlocks, different speeds or malfunctions. \\
For this reason we considered the possibility to force agents to only decide and learn in switches, where multiple actions are allowed and agents may learn to give way to other agents, avoid deadlocks, reach the target and more. This considerations lead to skip a lot of choices during learning and deploy Action Masking to avoid illegal actions in fact some studies have proposed to deploy action masking to avoid the selection of multiple actions when they are not necessary. A very common strategy to address invalid actions is applying negative rewards, but this also requires the agent to explore the actions and understand how to map actions to the possibility of applying them. During this period it is possible that the agent converges to a wrong policy. Invalid action masking helps to avoid sampling invalid actions by “masking out” the network outcomes corresponding to the invalid actions. \\
Our approach is been to evaluate the action only before or on the switch and in these case there is a flag "decision$\_$required" equals true, otherwise all other case this flag is false. 
\begin{itemize}
\item \textbf{"decision$\_$required" = True}: In this case, action is decided by policy on based the observation.
\item \textbf{"decision$\_$required" = False}: In this case, action is decided by environment and it is constant "RailEnvActions.MOVE$\_$FORWARD" so the agent will continue on the road that it has begun to run across in the same direction.
\end{itemize}
\section{Deadlock Controller}
\section{Rewards}
	