\textbf{Reinforcement learning} aims at training agents that operate in an environment by assigning rewards to their actions. Agents decide which action to take based on a \textbf{policy} $\pi(a_t, s_t)$, with $s_t$ being the observation of the environment at time step t. \\
Maximizing the sum of rewards is the method whereby the agent improves the policy and learns.

\section{DQN}
\cite{dqn} described a network model to approximate the Q-function $Q^{\pi}(s,a)$, which measures, for each state-action pair, the discounted sum of rewards, following from the policy $\pi$. The optimal Q-function, that is the maximum reward which can be obtained by performing a certain action a in state s, obeys the \textbf{Bellman optimality equation} $$Q^*(s,a) = E[r + \gamma max_{a'}Q^*(s', a')]$$
It follows that the maximum return is obtained by the immediate reward and the discounted return that the agent gets by following the policy; in practice, however, neural networks are needed in order to avoid computing a huge Q-table, and they are trained by minimizing the \textbf{temporal difference} loss, that is the difference between the current prediction and the expected optimal target.  
We started by implementing a slight variation of DQN as described in \cite{dqn}, without the use of CNN, but by simply using fully connected layers with \textbf{ReLU}, which take as input a vector representation of the graph as processed by he GCN, since we felt that classic convolution wouldn't be very effective in this environment. \\
\begin{center}
\begin{neuralnetwork} [nodespacing=10mm, layerspacing=25mm,
			maintitleheight=2.5em, layertitleheight=2.5em,
			height=5, toprow=false, nodesize=17pt, style={},
			title={}, titlestyle={}]
		% nodespacing = vertical spacing between nodes
		% layerspacing = horizontal spacing between layers
		% maintitleheight = space reserved for main title
		% layertitleheight = space reserved for layer titles
		% height = max nodes in any one layer [REQUIRED]
		% toprow = top row in node space reserved for bias nodes
		% nodesize = size of nodes
		% style = style for internal "tikzpicture" environment
		% titlestyle = style for main title
		        \newcommand{\x}[2]{$s_#2$}
		        \newcommand{\y}[2]{$\hat{Q}_#2$}
		        \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
		        \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
		        \inputlayer[count=3, bias=true, title=Input\\layer, text=\x]
		        \hiddenlayer[count=4, bias=false, title=Hidden\\layer 1, text=\hfirst] \linklayers
		        \hiddenlayer[count=3, bias=false, title=Hidden\\layer 2, text=\hsecond] \linklayers
		        \outputlayer[count=5, title=Output\\layer, text=\y] \linklayers
\end{neuralnetwork}
\end{center}
	
\noindent 
The units in the hidden layers default to 64, but the dimensions and the number of layers are fully customizable by the user, while the input layer is just made by the state of the environment, and the output layer has as many units as there are actions in flatland. \\ The \textit{Model} class implements the neural network, using \textbf{Huber loss} and \textbf{Kaiming initialization}. The former is, as explained in \cite{huber}, a loss function that clips the gradient to the $[-1,1]$ interval, avoiding \textbf{exploding gradient}. While on one hand it behaves like the absolute loss, for small errors it is more similar to the squared loss, thus combining the properties of both losses, while circumventing the problem of outliers which plagues the squared loss.\\
\begin{figure}[H] 
\includegraphics[height=80mm, width=100mm, scale=0.5]{chapters/huber.png}
\centering
\caption{Huber and squared loss}
\label{fig:s2} 
\end{figure}
\noindent
 \\ Kaiming initialization is instead a method for weights matrix initialization, described in \cite{kaiming}, which draws samples from a standard normal distribution, to avoid \textbf{vanishing} and \textbf{exploding gradient}, and then adjusts this distribution to the ReLU activation function. It does so by doubling the variance, since ReLU halves it w.r.t. to the original standard normal distribution (see \ref{fig:s3}).
 
 \begin{figure}[H] 
\includegraphics[height=80mm, width=140mm, scale=0.5]{chapters/relu_dist.png}
\centering
\caption{Normal and ReLU distributions}
\label{fig:s3} 
\end{figure}

\noindent
\\
The \textit{DQNPolicy} class implements the main methods needed for the training loop:
\begin{itemize}
\item \textbf{act}: implements the \textbf{$\epsilon$-greedy} action selection policy, by generating a random number, and then comparing it with $\epsilon$. This hyperparameter is then iteratively decreased at the end of each episode in order to incrementally rise the probability of allowing the model itself to pick the action.
\item \textbf{step}: performs the learning step, though only once every \textit{update\_rate} time steps, a hyperparameter initialized in the yml configuration file. Before that, if prioritized experience replay is used, then the priority is computed and the current experience tuple stored in memory with that priority.
\item \textbf{learn}: called by the step method, it actually performs the learning step with an experience sampled by the memory. It computes the temporal difference target and then performs a gradient descent step. It also updates the memory with the newly computed temporal difference, which serves as a priority for the experiences.
\end{itemize}

\noindent
We also used a technique called \textbf{Fixed Q-targets}.\\
Since in a reinforcement learning setting we do not have target values prepared, we would need to use the same network for both the model's weights and for the target, leading to instability of the target values. Thus, we use 2 equal networks, one to just compute the target Q value, and the other for training. Every 100 time step, a tunable hyperparameter, the target network is \textit{soft updated} with a portion of the current weights determined by this formula $$\theta_i = \tau * \theta_i + (1 - \tau) * \theta_i$$ and by the hyperparameter $\tau$. 

\subsection{Experience replay}
Past experiences are recorded in a memory buffer that is sampled, when needed, to train the network. We implemented two different variations: \textbf{random} and \textbf{prioritized experience replay}, as explained in \cite{priority}. The paper illustrates greedy and stochastic prioritization: both leverage priority based on \textbf{temporal difference}, but the latter is more robust, allowing to replay every experience in the buffer at least once; instead, greedy prioritization risks overfitting since the same higher priority experiences are always replayed. \\
\noindent
Concretely, in stochastic prioritization, the probability to sample transition $i$ is $$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$ with $p_i = temporal\ difference\ + \epsilon$. \\
$\alpha$ is a hyperparameter that controls the tradeoff between random ($\alpha$ = 0) and priority choice ($\alpha$ = 1), while$\epsilon$ is a small positive hyperparameter to always keep a non-zero priority, thus allowing to revisit even the lowest priority experiences: this specific variant is called \textbf{proportional stochastic prioritization}. Finally, for further stability, we added \textbf{importance-sampling weights}: for each transition $i$, weights are computed as $$w_i = (\frac{1}{N} * \frac{1}{P(i)})^\beta$$ and then fed to the Q-learning update, giving higher importance to low priority transitions, thus fixing the inherent bias of priority sampling, that favors higher priority samples. $\beta$ is used to control the amount of sampling over time, and usually reaches 1 by the end, since it's more important to have unbiased updates near convergence.\\
\noindent
This implementation uses a \textit{sum tree} data structure to store priorities and experiences, a binary tree where the leaves contain the priorities and data, while the inner nodes feature the sum of the children's priorities, with the root having the total priority: the sum tree provides $O(log(n))$ updates and sampling, with an efficient way to retrieve cumulative sums.
\noindent

 \begin{figure}[H] 
\includegraphics[height=80mm, width=140mm, scale=0.5]{chapters/SumTree.png}
\centering
\caption{Sum Tree}
\label{fig:s4} 
\end{figure}
\noindent
The \textit{SumTree} class stores 2 numpy arrays, one for the experiences (so just the leaves), and the other represents the tree and thus stores the priorities. Inside the class there is a pointer to the current leaf that is constanly updated and used whenever a new experience is added to the replay memory. If the number of samples exceeds the capacity of the memory, the last experience is overwritten by the new one. \\
\textit{PrioritizedReplay} is the class implementing the probabilistic sampling of the experiences from the memory (sumtree): the total priority is divided in \textit{batch\_size} segments, so that for each element in the batch we have a segment from which we can sample uniformly a priority, which is in turn used to pick a leaf from the tree. 	

\subsection{Dueling}

The model was further improved with \textbf{dueling} architecture. The main issue of standard DQN lies in the fact that the \textbf{Bellman operator} tends to overestimate the Q value: as stated in \cite{dueling}, the solution seems to be to separate the computation of the Q value in 2 separate functions, $$Q_{\pi}(s,a) = V_{\pi}(s) + A_{\pi}(s,a) $$ The \textbf{advantage} $A(s,a)$, which computes the  ''advantage'' of taking the action a in state s w.r.t. the other possible actions in that state, and the \textbf{value} $V(s)$ which simply measures how good the state s is, regardless of the possible actions; this is particularly useful whenever we're dealing with environments that are not affected by actions in every state, like Flatland, where actions are only really relevant at switches.

\begin{figure}[H] 
\includegraphics[height=80mm, width=140mm, scale=0.5]{chapters/dueling.jpg}
\centering
\caption{Dueling DQN}
\label{fig:s5} 
\end{figure}
\noindent
Advantage and value still need to be recombined, and since the standard formula doesn't allow to extract A and V from the Q value, \cite{dueling} suggested to force the advantage to be 0 at the chosen best action, which results in the output formula for Q $$Q_{\pi}(s,a) = V_{\pi}(s) + (A_{\pi}(s,a) - max_{a' in A} A(s,a'))$$ It also highlights, however, that empirically, subtracting the average advantage yields better results.


\section{A2C}
Actor-critic algorithms, unlike \textbf{off-policy} methods, like Q-learning, rely on local updates and transitions, directly trying to learn the optimal policy.\\
Assuming to parametrize the policy $\pi$, A2C introduces a \textbf{baseline}, the critic network, dependent on the state of the environment, which is essentially the value function \textbf{$V^{\pi}(s_t)$}. Thus, the policy, computed by the actor network, is evaluated w.r.t to this baseline, such that the estimator function becomes $$\nabla_{\theta} log \pi(a_t | s_t,\theta) (R_t - V^{\pi}(s_t))$$ with the difference between the reward and the baseline measuring how well the actor performed. \\ \\
In \cite{a2c}, the authors implemented an A2C algorithm to solve a DAG scheduling problem. Since we also employed a DAG as observation, we decided to implement an actor-critic network with graph normalization, so as to feed the algorithm an acceptable representation of the graph. \\ 
The models are the actor and the critic: they share the first 2 Dense layers and then fork from there, the former has an output layer using \textit{softmax} activation function for action probabilities, while the latter has an output layer for the value function; the actor defines a custom loss function that computes the log likelihood baseline function, as described above.  We also utilized an auxiliary model specifically for action selection. Similarly to how DQN was implemented, there is an \textit{A2C} policy class that defines an act, step and learning method:
\begin{itemize}
\item \textbf{act}: action selection is not done here through an $\epsilon$ hyperparameter, but by a probabilistic pick of an action.
\item \textbf{step}: calls the learning method every \textit{update\_rate} timesteps.
\item \textbf{learn}: computes the difference between the next predicted reward and the baseline value, sets it in the model where the custom loss function is defined, and then trains both the actor and the critic.
\end{itemize}

 